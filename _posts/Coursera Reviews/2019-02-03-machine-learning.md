---
layout: default
title:  "Machine Learning"
date:   2019-02-03
categories: [Coursera Reviews]
---
# {{page.title}} - {{page.categories}}

So I recently finished this course on Coursera after having started  around the ring of the new year. “Wow, so fast!”…well, not really, as  I’d tinkered with a lot of the ideas in the class already. I thought  it’d be a good primer for the Deep Learning specialization, and while  that is not untrue I was actually pleasantly surprised by a few things  about this course.

First of all I learned a decent amount of math. Not in the sense that  there was any notation I didn’t understand or high-level concepts for  the topics I’d seen I didn’t understand, but the nitty-gritty of nearly  every model I saw was *clarified* in a way I didn’t expect. One  of my only wishes was that the class included a bit more of the  mathematical rigor, but I think a class or textbook in linear algebra  ought to do the trick. Beyond that I had almost no critiques for the  class-even Matlab, which I thought would be an overly-tooled pain,  turned out to be an excellent prototyping tool as was stated. I decided  to switch to Octave afterwards because it is a much smaller footprint  and it is obviously a free tool, but I was pleasantly surprised at how a  mathematical modelling system helped bring clarity while I was  designing more math-focused algorithms. This certainly is not the  approach for every program (and indeed some I’ve written for other  courses the most advanced math was C buffers), but for these kind of  algorithms where small but powerful mathematical ideas guide the whole  program and can be easily tweaked, a prototyping system that allows  rapid dynamic change to the equations was a great tool.

As for the subject material it is presented in a candid manner, and a  pragmatic approach about how the methods can be applied, improved, and  where they simply don’t work (perhaps the most important of the three!)  were all presented with easy to conceptualize examples which consisted  of both functional notation and graphical representation to represent  them. I especially enjoyed the support vector machine and all of the  unsupervised learning topics, as these were the topics I had little to  no practical experience with. I was surprised that the amount of time it  took me to conceptually grasp and practically implement these topics  was almost no longer than the earlier topics with which I’d had more  experience, and I think Andrew Ng’s presentation style as well as  written notes to review had a lot to do with it.

I especially liked his approach to improving learning systems of this  type. Today it is easy to say “just throw more data at it” and assume  that it will work, but if something about the model isn’t working (in  the case of more data, perhaps we don’t have enough features to really  capture the variance of the problem) then just throwing more data at it  and spending a large amount of time or money doing it would not be a  wise choice. Conversely he also showed how little time 10,000 examples  could be easy to label, and in many cases this could significantly  increase the size of a data set. You would think that people who teach  computer systems about cost/benefit analysis would have a better sense  of it themselves, but the warnings in this class about trends seen shows  that it takes a skilled practitioner to know when and how to attack  trying to improve a machine learning model.

Overall I would definitely recommend auditing the course at the  minimum, and I can’t really speak to how the certificate will help or if  it will at all. Either way for the price it was when I took the course  at least (around $50), the price seemed well worth it for the amount of  quality lectures and notes received-compared to a college class this may  include slightly less mathematical rigor, but is probably 1/10 the  price or less. I suppose you could just go take a linear algebra course  with the 9/10ths you saved!