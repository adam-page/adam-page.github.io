---
layout: default
title:  "Week 6 Writeup"
date:   2019-02-18
category: [Defense Against the Dark Arts]

---

# {{page.title}}

#### [{{page.category}}](/blog/#{{page.category[0] | downcase | url_escape | strip | replace: ' ', '-' }})&mdash;{{ page.date | date: "%B %-d, %Y" }} 

â€”

This week in DADA was a refreshing change as there was a decent amount of code and many ideas that required knowledge from Computer Networks, which I believe is a prerequisite.

As an overall thought for the week I think Defense-in-Depth is the largest takeaway: no single measure for network security will ever work under our current internet systems, and as such one should always employ safe browsing, sound operating system practices, firewalls, and if necessary active packet profiling. This layered defense will ensure that no matter where a threat is caught, it will indeed be caught. Where one layer fails others will succeed, and this is the optimal strategy to combat the multitude of threats currently existing in the internet ecosystem.

Before I go into my most focused topic for this writeup I will react a bit again to the lab due the same time as this writeup, which focuses on gaining information about IP addresses, their protocol, and related information. The lab was interesting for starters because it was simply the most actual code I've had to write for this class yet, and while I wanted to do more data analysis the structure and nature of how the code had to be written was a bit hindering. The lab was actually quite fun and empowering, but developing on a VM seemed trivial as a list of comma separated values is not likely to be threatening. My biggest reason for wanting to develop locally is just that the list of tools available to us on the Linux VM were somewhat limited, and the Python libraries available were even more limited. All the same the assignment was fun, so I'll just try to remain focused on the things I learned from it.

The larger topic I'd like to discuss for this week is the Robustness Principle from RFC-1122. Lets start with this: "Software should be written to deal with every conceivable error..." To me that just seems utterly ridiculous. I understand the idea behind the robustness principle, and indeed the author here goes on to say that he specifically means that we should assume packets are going to be malevolent and treat them as such. While I agree with the latter point I just can't fathom how one can possibly even propose the former.

One of the interesting things I've found about software as I learn more about its development is the real explosion of possibilities that complex software can take. Indeed in discussing random testing we are obliged to talk about targeted random approaches, because once you introduce a sufficiently complex program (often not much is required) the number of conceivable random permutations takes an almost inconceivable amount of time to test. My choice of words is no error and to be completely clear I mean that I could very easily write software that I could conceive of a great many errors as a result, and that if I employed fully random input as testing input the amount of time taken to test it could take well over the span of my lifetime. We simply cannot program this way, and perhaps this is my issue with this statement. Otherwise I actually think the robustness principle is an excellent way to promote the spirit of the current internet, whether good or bad, while preserving as much functionality and safety for the system in question.

Perhaps part of the problem here is the assumption that all software is malevolent, but more practically the problem is likely that we are assuming that given any amount of malevolence (which can certainly be found on the internet) that the amount of harm will scale with this malevolence. Surely you think me optimistic, and I must admit that I use software downloaded from the internet, updated via the internet, on a Linux operating system released and updated through the internet; the threat of system compromise will always be there when the system itself is part of what could be malicious. To be clear though, one of the most fundamental part of many Linux package management systems is trusted repositories. The managers of the package managers for these operating systems build a repository based on trust, such that when a user tries to install a package not on this approved list, a warning is issued or the user must even add the repository themselves, inherently accepting the risk of "going off the grid". I don't use enough software on Windows to use the Microsoft-sanctioned Store and generally the handful or two of software packages I use on Windows come from sources I know are well-reputed, but nonetheless my assumption is that the approach is similar. So why does not the internet at large work like these repositories?

The software repositories I'm talking about in this question are a list of trusted sources, mostly known to be benign, which give the user confidence that their software should not elicit malicious responses as those the Robustness Principle warns us to guard against. Indeed for those getting system-level software in this way, if we can't trust the source in this manner then we have already lost the battle. So in that regard I find the Robustness Principle a little silly at best, and at worst it exposes a design flaw in our architecture of choice for the modern internet.

To reach a bit more broadly, my closing argument will be that we should probably just scrap things like the Robustness Principle in designing replacement systems for the modern internet. It is great considering the free-ranging ecosystem we currently exist in, but as a model for the future I find it too inclusive. I think this should range as much from content as it does to code, and indeed my biggest concerns about the content of the internet is that anyone can publish anything without any checks or balances, and those who tout everything from false information to morally reprehensible behavior all find a welcoming home on the modern internet. We should always subscribe to prevent server anonymity and protect user anonymity, and thus put the burden of content improvement on the server, or in this case lets call it publisher. Much like with books we should require that publishers stand by that which they publish, or more importantly that which others post on their servers. This last point is imperative in giving voices to those who cannot find one elsewhere, which is a common argument against the type of architecture I'm talking about. This is just the start of the discussion, but as I type this Facebook is starting to consider steering people away from health misinformation amid a measles outbreak, one article about it found on [CNN](https://www.cnn.com/2019/02/15/health/facebook-anti-vaccine-posts-bn/index.html).

As a final note I am not a proponent of silencing those minorities that would otherwise not find a voice. This is without question the most difficult part of the system I am talking about, but indeed we should foster network-based content that supports these types of groups while maintaining their anonymity, while not supporting groups that spread the misinformation and hate that are also currently poisoning many parts of the network.